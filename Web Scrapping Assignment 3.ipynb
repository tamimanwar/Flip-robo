{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9ba306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a product to search on Amazon: guitar\n",
      "Failed to fetch results.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_product(product):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    params = {\"k\": product}\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        product_elements = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "        for product_element in product_elements:\n",
    "            product_title = product_element.find(\"h2\").text.strip()\n",
    "            product_price = product_element.find(\"span\", class_=\"a-offscreen\")\n",
    "            if product_price:\n",
    "                product_price = product_price.text.strip()\n",
    "            else:\n",
    "                product_price = \"Price not available\"\n",
    "\n",
    "            print(f\"Product: {product_title}\\nPrice: {product_price}\\n\")\n",
    "    else:\n",
    "        print(\"Failed to fetch results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter a product to search on Amazon: \")\n",
    "    search_amazon_product(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932848cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a product to search on Amazon: Guitar\n",
      "Failed to fetch results for page 1.\n",
      "Failed to fetch results for page 2.\n",
      "Failed to fetch results for page 3.\n",
      "Data saved to amazon_product_details.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product_element):\n",
    "    product_title = product_element.find(\"h2\").text.strip()\n",
    "    product_price = product_element.find(\"span\", class_=\"a-offscreen\")\n",
    "    product_price = product_price.text.strip() if product_price else \"-\"\n",
    "    \n",
    "    return product_title, product_price\n",
    "\n",
    "def search_amazon_product(product, pages=3):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    product_details = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        params = {\"k\": product, \"page\": page}\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            product_elements = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "            for product_element in product_elements:\n",
    "                product_title, product_price = scrape_product_details(product_element)\n",
    "                product_details.append((product_title, product_price))\n",
    "        else:\n",
    "            print(f\"Failed to fetch results for page {page}.\")\n",
    "\n",
    "    return product_details\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter a product to search on Amazon: \")\n",
    "    product_data = search_amazon_product(user_input)\n",
    "\n",
    "    df = pd.DataFrame(product_data, columns=[\"Product Name\", \"Price\"])\n",
    "    df.to_csv(\"amazon_product_details.csv\", index=False)\n",
    "\n",
    "    print(\"Data saved to amazon_product_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03610572",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_css_selector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29564\\1011345289.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# You should provide the path to your Chrome driver executable here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mscrape_images_for_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29564\\1011345289.py\u001b[0m in \u001b[0;36mscrape_images_for_keywords\u001b[1;34m(driver, keywords, num_images)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Find and click the \"Show more results\" button\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mshow_more_button\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".mye4qd\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mshow_more_button\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_css_selector'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def scrape_images_for_keywords(driver, keywords, num_images=10):\n",
    "    for keyword in keywords:\n",
    "        search_url = f\"https://www.google.com/search?q={keyword}&tbm=isch\"\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Scroll down to load more images (simulate user interaction)\n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollBy(0, 1000)\")\n",
    "\n",
    "        # Find and click the \"Show more results\" button\n",
    "        show_more_button = driver.find_element_by_css_selector(\".mye4qd\")\n",
    "        show_more_button.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get image URLs\n",
    "        image_elements = driver.find_elements_by_css_selector(\".rg_i\")\n",
    "        image_urls = [image.get_attribute(\"src\") for image in image_elements]\n",
    "\n",
    "        # Save images\n",
    "        for i, image_url in enumerate(image_urls[:num_images]):\n",
    "            image_data = requests.get(image_url).content\n",
    "            with open(f\"{keyword}_{i+1}.jpg\", \"wb\") as f:\n",
    "                f.write(image_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\"fruits\", \"cars\", \"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "    \n",
    "    driver = webdriver.Chrome()  # You should provide the path to your Chrome driver executable here\n",
    "    \n",
    "    scrape_images_for_keywords(driver, keywords, num_images=10)\n",
    "    \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphone_details(driver, search_keyword):\n",
    "    search_url = f\"https://www.flipkart.com/search?q={search_keyword}\"\n",
    "    driver.get(search_url)\n",
    "\n",
    "    details_list = []\n",
    "\n",
    "    product_cards = driver.find_elements_by_css_selector(\"._1AtVbE\")\n",
    "    for card in product_cards:\n",
    "        try:\n",
    "            brand = card.find_element_by_css_selector(\"._2WkVRV\").text\n",
    "        except:\n",
    "            brand = \"-\"\n",
    "\n",
    "        try:\n",
    "            name = card.find_element_by_css_selector(\"._4rR01T\").text\n",
    "        except:\n",
    "            name = \"-\"\n",
    "\n",
    "        try:\n",
    "            color = card.find_element_by_css_selector(\"._2i24CF\").text\n",
    "        except:\n",
    "            color = \"-\"\n",
    "\n",
    "        try:\n",
    "            specs = card.find_element_by_css_selector(\"._3ULzGw\").text.split(\"\\n\")\n",
    "            ram = specs[0] if len(specs) > 0 else \"-\"\n",
    "            storage = specs[1] if len(specs) > 1 else \"-\"\n",
    "            primary_camera = specs[2] if len(specs) > 2 else \"-\"\n",
    "            secondary_camera = specs[3] if len(specs) > 3 else \"-\"\n",
    "            display_size = specs[4] if len(specs) > 4 else \"-\"\n",
    "            battery_capacity = specs[5] if len(specs) > 5 else \"-\"\n",
    "        except:\n",
    "            ram, storage, primary_camera, secondary_camera, display_size, battery_capacity = \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"\n",
    "\n",
    "        try:\n",
    "            price = card.find_element_by_css_selector(\"._30jeq3\").text\n",
    "        except:\n",
    "            price = \"-\"\n",
    "\n",
    "        try:\n",
    "            product_url = card.find_element_by_css_selector(\"a\").get_attribute(\"href\")\n",
    "        except:\n",
    "            product_url = \"-\"\n",
    "\n",
    "        details_list.append([brand, name, color, ram, storage, primary_camera, secondary_camera, display_size,\n",
    "                            battery_capacity, price, product_url])\n",
    "\n",
    "    return details_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "    driver = webdriver.Chrome()  # You should provide the path to your Chrome driver executable here\n",
    "    \n",
    "    details_list = scrape_smartphone_details(driver, search_keyword)\n",
    "\n",
    "    df = pd.DataFrame(details_list, columns=[\"Brand\", \"Smartphone Name\", \"Colour\", \"RAM\", \"Storage(ROM)\",\n",
    "                                             \"Primary Camera\", \"Secondary Camera\", \"Display Size\",\n",
    "                                             \"Battery Capacity\", \"Price\", \"Product URL\"])\n",
    "    \n",
    "    df.to_csv(\"smartphone_details.csv\", index=False)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    print(\"Data saved to smartphone_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    base_url = \"https://www.google.com/maps/search/\"\n",
    "    search_url = f\"{base_url}{city_name}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        coordinates_element = soup.find(\"meta\", itemprop=\"image\")\n",
    "        \n",
    "        if coordinates_element:\n",
    "            coordinates = coordinates_element[\"content\"].split(\";\")\n",
    "            latitude = coordinates[1].split(\"=\")[1]\n",
    "            longitude = coordinates[2].split(\"=\")[1]\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the city name: \")\n",
    "    latitude, longitude = get_coordinates(city_name)\n",
    "    \n",
    "    if latitude and longitude:\n",
    "        print(f\"Coordinates of {city_name}: Latitude = {latitude}, Longitude = {longitude}\")\n",
    "    else:\n",
    "        print(f\"Coordinates not found for {city_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        laptop_list = []\n",
    "\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading')\n",
    "        for laptop in laptops:\n",
    "            laptop_details = laptop.find_next('div', class_='TopStoryCont')\n",
    "            laptop_name = laptop_details.find('h3').text.strip()\n",
    "            specs = laptop_details.find_all('td')\n",
    "            processor = specs[0].text.strip()\n",
    "            ram = specs[1].text.strip()\n",
    "            os = specs[2].text.strip()\n",
    "            storage = specs[3].text.strip()\n",
    "\n",
    "            laptop_info = {\n",
    "                'Laptop Name': laptop_name,\n",
    "                'Processor': processor,\n",
    "                'RAM': ram,\n",
    "                'Operating System': os,\n",
    "                'Storage': storage\n",
    "            }\n",
    "            laptop_list.append(laptop_info)\n",
    "\n",
    "        return laptop_list\n",
    "    else:\n",
    "        print('Failed to retrieve the page.')\n",
    "        return []\n",
    "\n",
    "gaming_laptops = scrape_gaming_laptops()\n",
    "\n",
    "if gaming_laptops:\n",
    "    for laptop in gaming_laptops:\n",
    "        print('Laptop:', laptop['Laptop Name'])\n",
    "        print('Processor:', laptop['Processor'])\n",
    "        print('RAM:', laptop['RAM'])\n",
    "        print('Operating System:', laptop['Operating System'])\n",
    "        print('Storage:', laptop['Storage'])\n",
    "        print('-----------------------------')\n",
    "else:\n",
    "    print('No data available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e059cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "def get_youtube_comments(video_id, max_comments=500):\n",
    "    api_key = 'AIzaSyCGwfe0waks0T3G1RfigQUOYioGM4fCxGo'  # Replace with your actual YouTube API key\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    comments = []\n",
    "    nextPageToken = None\n",
    "\n",
    "    while len(comments) < max_comments:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=min(max_comments - len(comments), 100),\n",
    "            pageToken=nextPageToken\n",
    "        ).execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = comment['textDisplay']\n",
    "            comment_upvotes = comment['likeCount']\n",
    "            comment_time = comment['publishedAt']\n",
    "\n",
    "            comments.append({\n",
    "                'Comment': comment_text,\n",
    "                'Upvotes': comment_upvotes,\n",
    "                'Timestamp': comment_time\n",
    "            })\n",
    "\n",
    "        nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "video_id = 'ad79nYk2keg&ab_channel=Simplilearn'  # Replace with the video's actual ID\n",
    "comments = get_youtube_comments(video_id, max_comments=500)\n",
    "\n",
    "for comment in comments:\n",
    "    print('Comment:', comment['Comment'])\n",
    "    print('Upvotes:', comment['Upvotes'])\n",
    "    print('Timestamp:', comment['Timestamp'])\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels(location):\n",
    "    base_url = f'https://www.hostelworld.com/search?search_keywords={location}&country=England&city=London&date_from=2023-08-01&date_to=2023-08-05'\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        hostels_list = []\n",
    "\n",
    "        hostels = soup.find_all('div', class_='property-card')\n",
    "        for hostel in hostels:\n",
    "            name = hostel.find('h2', class_='title').text.strip()\n",
    "            distance = hostel.find('span', class_='description').text.strip()\n",
    "            rating = hostel.find('div', class_='score orange big').text.strip()\n",
    "            total_reviews = hostel.find('div', class_='reviews')\n",
    "            total_reviews = total_reviews.find('div', class_='keyword').text.strip()\n",
    "            overall_reviews = hostel.find('div', class_='score gray').text.strip()\n",
    "            prices = hostel.find_all('div', class_='price-col')\n",
    "            privates_from = prices[0].find('span').text.strip()\n",
    "            dorms_from = prices[1].find('span').text.strip()\n",
    "            facilities = ', '.join([item.text.strip() for item in hostel.find_all('div', class_='facilities')])\n",
    "            description = hostel.find('div', class_='text').text.strip()\n",
    "\n",
    "            hostel_info = {\n",
    "                'Name': name,\n",
    "                'Distance from City Centre': distance,\n",
    "                'Rating': rating,\n",
    "                'Total Reviews': total_reviews,\n",
    "                'Overall Reviews': overall_reviews,\n",
    "                'Privates from Price': privates_from,\n",
    "                'Dorms from Price': dorms_from,\n",
    "                'Facilities': facilities,\n",
    "                'Description': description\n",
    "            }\n",
    "            hostels_list.append(hostel_info)\n",
    "\n",
    "        return hostels_list\n",
    "    else:\n",
    "        print('Failed to retrieve the page.')\n",
    "        return []\n",
    "\n",
    "location = 'London'\n",
    "hostels = scrape_hostels(location)\n",
    "\n",
    "if hostels:\n",
    "    for hostel in hostels:\n",
    "        print('Name:', hostel['Name'])\n",
    "        print('Distance from City Centre:', hostel['Distance from City Centre'])\n",
    "        print('Rating:', hostel['Rating'])\n",
    "        print('Total Reviews:', hostel['Total Reviews'])\n",
    "        print('Overall Reviews:', hostel['Overall Reviews'])\n",
    "        print('Privates from Price:', hostel['Privates from Price'])\n",
    "        print('Dorms from Price:', hostel['Dorms from Price'])\n",
    "        print('Facilities:', hostel['Facilities'])\n",
    "        print('Description:', hostel['Description'])\n",
    "        print('-----------------------------')\n",
    "else:\n",
    "    print('No hostels data available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25437176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
